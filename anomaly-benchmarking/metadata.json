{
  "name": "Anomaly benchmarking",
  "description": "- Point difficulty: Models the dataset with a logistic regression, and
  uses a batchprediction to labels each row with the probability of it
  being labeled normal. The rows labeled anomalous are sorted into
  one of four datasets based on this probability. (easy (0, 0.1666),
  medium [0.1666, 0.3333), hard [0.3333, 0.5), very hard [0.5,
  1))

- Clusteredness: For each of the point difficulty datasets, an anomaly
  detector (isolation forest) and batchprediction are used to label
  each row with an anomaly score. This is a rough gauge for
  clusteredness, and new datasets are created from the top and bottom
  20% anomaly scores. The actual clusteredness is calculated by
  finding the ratio of the variance of the normal rows to the
  variance of the new dataset (which has N rows). Actual clusteredness
  is sorted into one of six categories: high scatter (0, 0.25), medium
  scatter [0.25, 0.5), low scatter [0.5, 1), low clusteredness [1,2),
  medium clusteredness [2, 4), and high clusteredness [4, infinity).

- Frequency: As long as the dataset has at least 10 rows, a new
  dataset (which has K rows) is created through a sample rate set to
  create the goal frequency. Goal frequencies are 0.001, 0.005, 0.01,
  0.05, and 0.1.

- If enough rows are availible, ten replicate datasets are created at
  each combination of difficulty, clusteredness, and frequency. A
  maxiumum of N/K replicates are created.
",
  "kind": "package",
  "components": [
      "make-binary",
      "generate-datasets"
  ]
}
